{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvLyprhIdzJjyc3GBScgcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenzhonghuangjun/Bike-sharing/blob/main/Repayment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Merge datasets##"
      ],
      "metadata": {
        "id": "rN6d33F4YiOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP36soWQSVLo",
        "outputId": "c3755e01-18fb-43ab-94df-81cb9138e51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the paths to the datasets\n",
        "file_names = [\n",
        "    'Assets_2020.csv',\n",
        "    'Baseline_survey_data.csv',\n",
        "    'BRAC_arrears_data.csv',\n",
        "    'BRAC_default_data.csv',\n",
        "    'BRAC_eligibles_loanV_data.csv',\n",
        "    'BRAC_voucher_data.csv',\n",
        "    'Endline_survey_data.csv',\n",
        "    'Midline_survey_data.csv',\n",
        "    'Rainfall.csv'\n",
        "]\n",
        "\n",
        "# Load all datasets into a dictionary of DataFrames\n",
        "datasets = {name: pd.read_csv(f'/content/drive/My Drive/Rep/Test/{name}') for name in file_names}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets from the previously defined dictionary\n",
        "baseline = datasets['Baseline_survey_data.csv']\n",
        "midline = datasets['Midline_survey_data.csv']\n",
        "endline = datasets['Endline_survey_data.csv']\n",
        "brac_loans = datasets['BRAC_eligibles_loanV_data.csv']\n",
        "\n",
        "# Append baseline, midline, and endline datasets\n",
        "# Add a 'followup' column to distinguish between the datasets\n",
        "baseline['followup'] = 'baseline'\n",
        "midline['followup'] = 'midline'\n",
        "endline['followup'] = 'endline'\n",
        "\n",
        "# Combine the datasets by appending them\n",
        "panel_temp = pd.concat([baseline, midline, endline], ignore_index=True)\n",
        "\n",
        "# Sort the combined dataset by 'id' and 'followup'\n",
        "panel_temp.sort_values(by=['id', 'followup'], inplace=True)\n",
        "\n",
        "# Ensure the 'followup' column in both DataFrames is of the same type (string)\n",
        "panel_temp['followup'] = panel_temp['followup'].astype(str)\n",
        "brac_loans['followup'] = brac_loans['followup'].astype(str)\n",
        "\n",
        "# Merge with the BRAC loans data on 'id' and 'followup'\n",
        "merged_data = pd.merge(panel_temp, brac_loans, on=['id', 'followup'], how='left')\n",
        "\n",
        "# Optionally, check the merge status (though pandas does not create a _merge column by default)\n",
        "# This would have been done with _merge in Stata, but pandas requires specifying it\n",
        "merge_status = merged_data.isna().sum()\n",
        "print(\"Merge status by column (NaN counts):\")\n",
        "print(merge_status)\n",
        "\n",
        "# Filter the dataset for 'dabi' or 'progoti' borrowers\n",
        "filtered_data = merged_data[(merged_data['dabi'] == 1) | (merged_data['progoti'] == 1)]\n",
        "\n",
        "# Drop any columns that you don't need (like the merge indicator if manually added)\n",
        "# For example, if you had a column _merge, you'd drop it here\n",
        "# filtered_data.drop(columns=['_merge'], inplace=True)\n",
        "\n",
        "# Save the final dataset to a CSV file\n",
        "filtered_data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp.csv', index=False)\n",
        "\n",
        "print(\"Data processing complete. The final dataset has been saved to 'panel_temp.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGTmUaUbaQxp",
        "outputId": "3d9d9648-aa9a-4f0f-ca32-95634bb56ab8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge status by column (NaN counts):\n",
            "id                       0\n",
            "region                   1\n",
            "identifier_branch        1\n",
            "brac_loanee_vo_no    15561\n",
            "followup                 0\n",
            "                     ...  \n",
            "wage_inc_resp           11\n",
            "int_done              6245\n",
            "BRACLoanD            18844\n",
            "BRACLoanValue        18844\n",
            "Default_y            18844\n",
            "Length: 157, dtype: int64\n",
            "Data processing complete. The final dataset has been saved to 'panel_temp.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Rainfall dataset\n",
        "rainfall = datasets['Rainfall.csv']\n",
        "\n",
        "# Generate 'jan_dec' variables for the years 2013 and 2014\n",
        "for y in range(2013, 2015):\n",
        "    rainfall[f'jan_dec{y}'] = (\n",
        "        rainfall[f'Rain{y}1'] + rainfall[f'Rain{y}2'] + rainfall[f'Rain{y}3'] +\n",
        "        rainfall[f'Rain{y}4'] + rainfall[f'Rain{y}5'] + rainfall[f'Rain{y}6'] +\n",
        "        rainfall[f'Rain{y}7'] + rainfall[f'Rain{y}8'] + rainfall[f'Rain{y}9'] +\n",
        "        rainfall[f'Rain{y}10'] + rainfall[f'Rain{y}11'] + rainfall[f'Rain{y}12']\n",
        "    )\n",
        "\n",
        "# Generate 'dec_may' variables for the periods from 1983-1990 and 1992-2016\n",
        "for y in range(1983, 1991):\n",
        "    x = y + 1\n",
        "    rainfall[f'dec_may{x}'] = (\n",
        "        rainfall[f'dec{y}'] + rainfall[f'jan{x}'] + rainfall[f'feb{x}'] +\n",
        "        rainfall[f'mar{x}'] + rainfall[f'apr{x}'] + rainfall[f'may{x}']\n",
        "    )\n",
        "\n",
        "# Handle the special case for the year 1992 (rainfall is missing for February 1992)\n",
        "rainfall['dec_may1992'] = (\n",
        "    rainfall['dec1991'] + rainfall['jan1992'] + rainfall['mar1992'] +\n",
        "    rainfall['apr1992'] + rainfall['may1992']\n",
        ")\n",
        "\n",
        "# Continue generating 'dec_may' variables from 1992-2016\n",
        "for y in range(1992, 2017):\n",
        "    x = y + 1\n",
        "    rainfall[f'dec_may{x}'] = (\n",
        "        rainfall[f'dec{y}'] + rainfall[f'jan{x}'] + rainfall[f'feb{x}'] +\n",
        "        rainfall[f'mar{x}'] + rainfall[f'apr{x}'] + rainfall[f'may{x}']\n",
        "    )\n",
        "\n",
        "# Save the cleaned and aggregated dataset\n",
        "rainfall.to_csv('/content/drive/My Drive/Rep/Test/Rainfall_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Rainfall data processing complete. The cleaned dataset has been saved as 'Rainfall_cleaned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K58m0FRUbOij",
        "outputId": "1ede6952-1918-4f2a-e950-dd54f7abc699"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rainfall data processing complete. The cleaned dataset has been saved as 'Rainfall_cleaned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate historical means and standard deviations for December to May (1984-2015, 1984-2016, 1984-2017)\n",
        "years = list(range(1984, 2016 + 1))  # All years from 1984 to 2016\n",
        "\n",
        "# Mean and SD for 2016 (using data up to 2015)\n",
        "rainfall['dec_maymean15'] = rainfall[[f'dec_may{year}' for year in years[:-1]]].mean(axis=1)\n",
        "rainfall['dec_maysd15'] = rainfall[[f'dec_may{year}' for year in years[:-1]]].std(ddof=0, axis=1)\n",
        "\n",
        "# Mean and SD for 2017 (using data up to 2016)\n",
        "rainfall['dec_maymean16'] = rainfall[[f'dec_may{year}' for year in years]].mean(axis=1)\n",
        "rainfall['dec_maysd16'] = rainfall[[f'dec_may{year}' for year in years]].std(ddof=0, axis=1)\n",
        "\n",
        "# Mean and SD for 2017 (using data up to 2017)\n",
        "years.append(2017)\n",
        "rainfall['dec_maymean17'] = rainfall[[f'dec_may{year}' for year in years]].mean(axis=1)\n",
        "rainfall['dec_maysd17'] = rainfall[[f'dec_may{year}' for year in years]].std(ddof=0, axis=1)\n",
        "\n",
        "# Standardize the rainfall for 2016 and 2017\n",
        "rainfall['SD_dec_may2016'] = (rainfall['dec_may2016'] - rainfall['dec_maymean15']) / rainfall['dec_maysd15']\n",
        "rainfall['SD_dec_may2017'] = (rainfall['dec_may2017'] - rainfall['dec_maymean16']) / rainfall['dec_maysd16']\n",
        "\n",
        "# Save the updated rainfall dataset\n",
        "rainfall.to_csv('/content/drive/My Drive/Rep/Test/Rainfall_cleaned_with_means.csv', index=False)\n",
        "\n",
        "print(\"Rainfall data processing complete. The dataset with historical means and standard deviations has been saved as 'Rainfall_cleaned_with_means.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5s3IegmcFNK",
        "outputId": "cc8b268d-4824-4136-e846-814a72f4760e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rainfall data processing complete. The dataset with historical means and standard deviations has been saved as 'Rainfall_cleaned_with_means.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate a 1-standard deviation shock based on the continuous measure\n",
        "rainfall['SD_dec_may2016_1'] = (rainfall['SD_dec_may2016'] > 1).astype(int)\n",
        "rainfall['SD_dec_may2017_1'] = (rainfall['SD_dec_may2017'] > 1).astype(int)\n",
        "\n",
        "# Save the updated rainfall dataset with the new binary columns\n",
        "rainfall.to_csv('/content/drive/My Drive/Rep/Test/Rainfall_with_SD_shocks.csv', index=False)\n",
        "\n",
        "print(\"Rainfall data processing complete. The dataset with 1-standard deviation shocks has been saved as 'Rainfall_with_SD_shocks.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev-8p6tbcuL3",
        "outputId": "2ef441aa-1d2d-4786-a134-bbb618b373fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rainfall data processing complete. The dataset with 1-standard deviation shocks has been saved as 'Rainfall_with_SD_shocks.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the relevant columns\n",
        "rainfall_relevant = rainfall[['identifier_branch', 'jan_dec2013', 'jan_dec2014', 'SD_dec_may2016_1', 'SD_dec_may2017_1']]\n",
        "\n",
        "# Sort the DataFrame by 'identifier_branch'\n",
        "rainfall_relevant = rainfall_relevant.sort_values(by='identifier_branch')\n",
        "\n",
        "# Save the filtered and sorted dataset\n",
        "rainfall_relevant.to_csv('/content/drive/My Drive/Rep/Test/Rainfall_relevant.csv', index=False)\n",
        "\n",
        "print(\"Relevant rainfall data saved as 'Rainfall_relevant.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMCtYfqDdLh7",
        "outputId": "40d74ac3-37e8-4cfe-ebb3-bb5375537d59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant rainfall data saved as 'Rainfall_relevant.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the rainfall data with the survey data using 'identifier_branch'\n",
        "merged_data = pd.merge(panel_temp, rainfall_relevant, on='identifier_branch', how='outer')\n",
        "\n",
        "# Display merge status if necessary\n",
        "merge_status = merged_data.isna().sum()\n",
        "print(\"Merge status by column (NaN counts):\")\n",
        "print(merge_status)\n",
        "\n",
        "# Filter rows where (dabi == 1) or (progoti == 1)\n",
        "filtered_data = merged_data[(merged_data['dabi'] == 1) | (merged_data['progoti'] == 1)]\n",
        "\n",
        "# Optionally, drop the '_merge' column if it was created manually\n",
        "# However, in pandas, the '_merge' column isn't created automatically unless specified\n",
        "# For example, if '_merge' was manually created or exists:\n",
        "# filtered_data.drop(columns=['_merge'], inplace=True)\n",
        "\n",
        "# Save the final merged and filtered dataset\n",
        "filtered_data.to_csv('/content/drive/My Drive/Rep/Test/merged_survey_rainfall.csv', index=False)\n",
        "\n",
        "print(\"Survey and rainfall data merged successfully and saved as 'merged_survey_rainfall.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBGkAenQdU4h",
        "outputId": "8df0c7dc-e6ab-493b-84e4-2446dc668d14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge status by column (NaN counts):\n",
            "id                       0\n",
            "region                   1\n",
            "identifier_branch        1\n",
            "brac_loanee_vo_no    15561\n",
            "followup                 0\n",
            "                     ...  \n",
            "int_done              6245\n",
            "jan_dec2013              1\n",
            "jan_dec2014              1\n",
            "SD_dec_may2016_1         1\n",
            "SD_dec_may2017_1         1\n",
            "Length: 158, dtype: int64\n",
            "Survey and rainfall data merged successfully and saved as 'merged_survey_rainfall.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "merged_data = pd.read_csv('/content/drive/My Drive/Rep/Test/merged_survey_rainfall.csv')\n",
        "\n",
        "# Initialize the new column `SD_dec_may_1` to NaN (or any default value)\n",
        "merged_data['SD_dec_may_1'] = np.nan\n",
        "\n",
        "# Assign values based on the followup column\n",
        "merged_data.loc[merged_data['followup'] == 1, 'SD_dec_may_1'] = merged_data['SD_dec_may2016_1']\n",
        "merged_data.loc[merged_data['followup'] == 2, 'SD_dec_may_1'] = merged_data['SD_dec_may2017_1']\n",
        "\n",
        "# Save the updated DataFrame\n",
        "merged_data.to_csv('/content/drive/My Drive/Rep/Test/merged_survey_rainfall_with_SD.csv', index=False)\n",
        "\n",
        "print(\"Rainfall data matched with survey data and saved as 'merged_survey_rainfall_with_SD.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU6fMfHldprI",
        "outputId": "0de85e98-fb59-4a66-d1b6-9a1a2b0d4754"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rainfall data matched with survey data and saved as 'merged_survey_rainfall_with_SD.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the data is loaded\n",
        "merged_data = pd.read_csv('/content/drive/My Drive/Rep/Test/merged_survey_rainfall_with_SD.csv')\n",
        "\n",
        "# Create quartiles for jan_dec2013 and jan_dec2014 for dabi group\n",
        "merged_data['jan_dec2013_4'] = np.nan\n",
        "merged_data['jan_dec2014_4'] = np.nan\n",
        "merged_data['jan_dec2013_4_p'] = np.nan\n",
        "merged_data['jan_dec2014_4_p'] = np.nan\n",
        "\n",
        "# Apply quartile computation only for dabi == 1\n",
        "dabi_mask = merged_data['dabi'] == 1\n",
        "merged_data.loc[dabi_mask, 'jan_dec2013_4'] = pd.qcut(merged_data.loc[dabi_mask, 'jan_dec2013'], q=4, labels=False, duplicates='drop') + 1\n",
        "merged_data.loc[dabi_mask, 'jan_dec2014_4'] = pd.qcut(merged_data.loc[dabi_mask, 'jan_dec2014'], q=4, labels=False, duplicates='drop') + 1\n",
        "\n",
        "# Apply quartile computation only for progoti == 1\n",
        "progoti_mask = merged_data['progoti'] == 1\n",
        "merged_data.loc[progoti_mask, 'jan_dec2013_4_p'] = pd.qcut(merged_data.loc[progoti_mask, 'jan_dec2013'], q=4, labels=False, duplicates='drop') + 1\n",
        "merged_data.loc[progoti_mask, 'jan_dec2014_4_p'] = pd.qcut(merged_data.loc[progoti_mask, 'jan_dec2014'], q=4, labels=False, duplicates='drop') + 1\n",
        "\n",
        "# Create dummy variables based on the quartile columns\n",
        "dum1 = pd.get_dummies(merged_data['jan_dec2014_4'], prefix='dum1')\n",
        "dum2 = pd.get_dummies(merged_data['jan_dec2013_4'], prefix='dum2')\n",
        "dum1_p = pd.get_dummies(merged_data['jan_dec2014_4_p'], prefix='dum1_p')\n",
        "dum2_p = pd.get_dummies(merged_data['jan_dec2013_4_p'], prefix='dum2_p')\n",
        "\n",
        "# Concatenate the dummy variables with the original DataFrame\n",
        "merged_data = pd.concat([merged_data, dum1, dum2, dum1_p, dum2_p], axis=1)\n",
        "\n",
        "# Save the updated DataFrame\n",
        "merged_data.to_csv('/content/drive/My Drive/Rep/Test/merged_survey_rainfall_with_quartiles.csv', index=False)\n",
        "\n",
        "print(\"Quartiles and dummy variables created and saved as 'merged_survey_rainfall_with_quartiles.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQE6KNEveNfy",
        "outputId": "a3cccd7e-4ee6-4468-db17-35a24203255a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quartiles and dummy variables created and saved as 'merged_survey_rainfall_with_quartiles.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data if it's not already loaded\n",
        "merged_data = pd.read_csv('/content/drive/My Drive/Rep/Test/merged_survey_rainfall_with_quartiles.csv')\n",
        "\n",
        "# Step 1: Rename the rainfall shock column\n",
        "merged_data.rename(columns={'SD_dec_may_1': 'rain'}, inplace=True)\n",
        "\n",
        "# Step 2: Keep only the relevant columns\n",
        "# Using regex to match columns that start with 'dum1', 'dum2', 'dum1_p', 'dum2_p'\n",
        "relevant_columns = ['id', 'rain', 'followup'] + \\\n",
        "                   [col for col in merged_data.columns if col.startswith('dum1') or col.startswith('dum2')]\n",
        "\n",
        "# Filter the DataFrame to keep only the relevant columns\n",
        "final_rain_data = merged_data[relevant_columns]\n",
        "\n",
        "# Step 3: Sort the DataFrame by 'id' and 'followup'\n",
        "final_rain_data = final_rain_data.sort_values(by=['id', 'followup'])\n",
        "\n",
        "# Step 4: Save the final DataFrame to a CSV file\n",
        "final_rain_data.to_csv('/content/drive/My Drive/Rep/Test/Rainshock_data.csv', index=False)\n",
        "\n",
        "print(\"Rainshock data prepared and saved as 'Rainshock_data.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aug7NUkUfHKR",
        "outputId": "93c13188-c886-40ca-d138-41f7beffe00b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rainshock data prepared and saved as 'Rainshock_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load the panel_temp data and Rainshock_data\n",
        "panel_temp = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp.csv')\n",
        "rainshock_data = pd.read_csv('/content/drive/My Drive/Rep/Test/Rainshock_data.csv')\n",
        "\n",
        "# Step 1: Sort both datasets by 'id' and 'followup'\n",
        "panel_temp = panel_temp.sort_values(by=['id', 'followup'])\n",
        "rainshock_data = rainshock_data.sort_values(by=['id', 'followup'])\n",
        "\n",
        "# Step 2: Merge the datasets on 'id' and 'followup'\n",
        "merged_data = pd.merge(panel_temp, rainshock_data, on=['id', 'followup'], how='outer', indicator=True)\n",
        "\n",
        "# Step 3: Display the merge result\n",
        "print(merged_data['_merge'].value_counts())\n",
        "\n",
        "# Step 4: Filter rows where either 'dabi' == 1 or 'progoti' == 1\n",
        "filtered_data = merged_data[(merged_data['dabi'] == 1) | (merged_data['progoti'] == 1)]\n",
        "\n",
        "# Step 5: Drop the '_merge' column\n",
        "filtered_data.drop(columns=['_merge'], inplace=True)\n",
        "\n",
        "# Step 6: Save the final dataset\n",
        "filtered_data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_updated.csv', index=False)\n",
        "\n",
        "# Step 7: Delete the temporary file (equivalent to Stata's erase command)\n",
        "os.remove('/content/drive/My Drive/Rep/Test/panel_temp.csv')\n",
        "\n",
        "print(\"Merged data processed and saved as 'panel_temp_updated.csv'. Temporary file 'panel_temp.csv' has been deleted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_syuaZK_fhQl",
        "outputId": "a32abb70-b771-400e-ebeb-a33fc5eceaa7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_merge\n",
            "both          7951\n",
            "left_only        0\n",
            "right_only       0\n",
            "Name: count, dtype: int64\n",
            "Merged data processed and saved as 'panel_temp_updated.csv'. Temporary file 'panel_temp.csv' has been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume that `filtered_data` is the merged dataset after previous operations\n",
        "# If it's not already loaded, load the dataset\n",
        "filtered_data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_updated.csv')\n",
        "\n",
        "# Step 1: Generate the attrition indicator for midline (followup == 1)\n",
        "filtered_data['x'] = 1 - filtered_data['int_done']\n",
        "filtered_data['x'] = filtered_data['x'].where(filtered_data['followup'] == 1, other=np.nan)\n",
        "\n",
        "# Group by 'id' and find the maximum value of 'x' (equivalent to `egen attrition = max(x)` in Stata)\n",
        "filtered_data['attrition'] = filtered_data.groupby('id')['x'].transform('max')\n",
        "\n",
        "# Drop the temporary 'x' column\n",
        "filtered_data.drop(columns=['x'], inplace=True)\n",
        "\n",
        "# Replace NaN values with 1 (equivalent to `replace attrition = 1 if attrition == .` in Stata)\n",
        "filtered_data['attrition'].fillna(1, inplace=True)\n",
        "\n",
        "# Label for 'attrition' (in comments, since pandas does not support labels)\n",
        "# attrition = 1 if not interviewed at midline\n",
        "\n",
        "# Step 2: Generate the attrition_end indicator for endline (followup == 2)\n",
        "filtered_data['x'] = 1 - filtered_data['int_done']\n",
        "filtered_data['x'] = filtered_data['x'].where(filtered_data['followup'] == 2, other=np.nan)\n",
        "\n",
        "# Group by 'id' and find the maximum value of 'x' for endline\n",
        "filtered_data['attrition_end'] = filtered_data.groupby('id')['x'].transform('max')\n",
        "\n",
        "# Drop the temporary 'x' column\n",
        "filtered_data.drop(columns=['x'], inplace=True)\n",
        "\n",
        "# Replace NaN values with 1 (equivalent to `replace attrition_end = 1 if attrition_end == .` in Stata)\n",
        "filtered_data['attrition_end'].fillna(1, inplace=True)\n",
        "\n",
        "# Label for 'attrition_end' (in comments, since pandas does not support labels)\n",
        "# attrition_end = 1 if not interviewed at endline\n",
        "\n",
        "# Save the updated dataset\n",
        "filtered_data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_updated_with_attrition.csv', index=False)\n",
        "\n",
        "print(\"Attrition indicators added and saved as 'panel_temp_updated_with_attrition.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "symfoL7fgPUs",
        "outputId": "eb9fabc4-fc56-4098-e6ca-1a701df9f227"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attrition indicators added and saved as 'panel_temp_updated_with_attrition.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_updated_with_attrition.csv')\n",
        "\n",
        "# Define the list of variables to be adjusted\n",
        "variables = [\n",
        "    'BRACLoanValue', 'FurnitureValue', 'InventoryValue', 'MachinesValue', 'OtherAssetValue',\n",
        "    'ProfitPm', 'Profits', 'RevBestM', 'RevWorstM', 'Revenues', 'SiteValue', 'ToolsValue',\n",
        "    'VehiclesValue', 'ave_furniture_val', 'ave_machine_val', 'ave_tool_val', 'ave_vehicle_val',\n",
        "    'cost_electricity', 'cost_fuel', 'cost_rent_mach', 'cost_rent_sites', 'cost_repairs',\n",
        "    'cost_stock', 'cost_trans', 'cost_wages', 'exp_ceremony', 'exp_clothes', 'exp_cosmetic',\n",
        "    'exp_dowry', 'exp_educ', 'exp_electricity', 'exp_entertain', 'exp_food', 'exp_fuel',\n",
        "    'exp_furniture', 'exp_maid', 'exp_other1', 'exp_other2', 'exp_other3', 'exp_trans',\n",
        "    'exp_utensils', 'hhasset_bike', 'hhasset_car', 'hhasset_cell', 'hhasset_fan', 'hhasset_fridge',\n",
        "    'hhasset_jewel', 'hhasset_motor', 'hhasset_oth1', 'hhasset_oth2', 'hhasset_oth3', 'hhasset_radio',\n",
        "    'hhasset_tv', 'land_rent_receive', 'wage_inc_hh', 'wage_inc_resp', 'profit_oth_bus',\n",
        "    'loan_given1', 'loan_given2', 'loan_given4', 'loan_given5', 'loan_given6', 'loan_given7',\n",
        "    'loan_given8', 'loan_given9', 'loan_given10', 'loan_given11', 'loan_given12', 'loan_given13',\n",
        "    'trans_given1', 'trans_given2', 'trans_given3', 'trans_given4', 'trans_given5',\n",
        "    'trans_received1', 'trans_received2', 'trans_received3', 'NbracloanV1', 'NbracloanV2',\n",
        "    'NbracloanV3', 'NbracloanV4', 'NbracloanV5', 'NbracloanV6', 'NbracloanV7', 'NbracloanV8',\n",
        "    'NbracloanV9', 'NbracloanV10', 'NbracloanV11', 'NbracloanV12'\n",
        "]\n",
        "\n",
        "# Constants\n",
        "inflation_rate = 0.0553\n",
        "usd_ppp_conversion = 28.25\n",
        "\n",
        "# Step 1: Deflate the midline values (followup == 1) and endline values (followup == 2)\n",
        "for var in variables:\n",
        "    # Deflate the midline values\n",
        "    data.loc[data['followup'] == 1, var] = data.loc[data['followup'] == 1, var] / (1 + inflation_rate)\n",
        "\n",
        "    # Deflate the endline values\n",
        "    data.loc[data['followup'] == 2, var] = data.loc[data['followup'] == 2, var] / ((1 + inflation_rate) ** 2)\n",
        "\n",
        "    # Step 2: Convert to USD PPP\n",
        "    data[var] = data[var] / usd_ppp_conversion\n",
        "\n",
        "# Save the updated DataFrame\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_adjusted_to_USD_PPP.csv', index=False)\n",
        "\n",
        "print(\"Monetary variables adjusted for inflation and converted to USD PPP. Data saved as 'panel_temp_adjusted_to_USD_PPP.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-r0N5mlhI03",
        "outputId": "ff6b8379-da7f-415b-cdb1-5c0e98b5e15c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monetary variables adjusted for inflation and converted to USD PPP. Data saved as 'panel_temp_adjusted_to_USD_PPP.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_adjusted_to_USD_PPP.csv')\n",
        "\n",
        "# Step 1: Calculate the total value of business assets\n",
        "# Using the row-wise sum across the specified columns\n",
        "data['BusinessAssetsValue'] = data[['InventoryValue', 'MachinesValue', 'OtherAssetValue',\n",
        "                                    'FurnitureValue', 'ToolsValue', 'VehiclesValue']].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label the variable (in comments, since pandas does not support variable labels)\n",
        "# BusinessAssetsValue = \"Value of business assets\"\n",
        "\n",
        "# Step 2: Generate summary statistics (equivalent to `su` in Stata)\n",
        "print(\"\\nSummary statistics for BusinessAssetsValue (Dabi = 1):\")\n",
        "print(data[data['dabi'] == 1]['BusinessAssetsValue'].describe())\n",
        "\n",
        "print(\"\\nSummary statistics for BusinessAssetsValue (Progoti = 1):\")\n",
        "print(data[data['progoti'] == 1]['BusinessAssetsValue'].describe())\n",
        "\n",
        "print(\"\\nSummary statistics for BusinessAssetsValue (Census = 1):\")\n",
        "print(data[data['census'] == 1]['BusinessAssetsValue'].describe())\n",
        "\n",
        "# Further breakdown by followup status\n",
        "print(\"\\nSummary statistics for BusinessAssetsValue (Dabi = 1 & followup = 0):\")\n",
        "print(data[(data['dabi'] == 1) & (data['followup'] == 0)]['BusinessAssetsValue'].describe())\n",
        "\n",
        "print(\"\\nSummary statistics for BusinessAssetsValue (Dabi = 1 & followup = 1):\")\n",
        "print(data[(data['dabi'] == 1) & (data['followup'] == 1)]['BusinessAssetsValue'].describe())\n",
        "\n",
        "print(\"\\nSummary statistics for BusinessAssetsValue (Dabi = 1 & followup = 2):\")\n",
        "print(data[(data['dabi'] == 1) & (data['followup'] == 2)]['BusinessAssetsValue'].describe())\n",
        "\n",
        "# Step 3: Handle outliers\n",
        "# Replacing the outlier with NaN if BusinessAssetsValue > 20,000,000\n",
        "data.loc[data['BusinessAssetsValue'] > 20000000, 'BusinessAssetsValue'] = np.nan\n",
        "\n",
        "# Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_BusinessAssetsValue.csv', index=False)\n",
        "\n",
        "print(\"Business assets value calculated and saved as 'panel_temp_with_BusinessAssetsValue.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1V9C3Yyhlnj",
        "outputId": "2573b505-bba0-42f7-84cb-56ca9d3b5345"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary statistics for BusinessAssetsValue (Dabi = 1):\n",
            "count    3.202000e+03\n",
            "mean     1.136979e+04\n",
            "std      3.763564e+05\n",
            "min      0.000000e+00\n",
            "25%      0.000000e+00\n",
            "50%      0.000000e+00\n",
            "75%      3.191947e+03\n",
            "max      2.125434e+07\n",
            "Name: BusinessAssetsValue, dtype: float64\n",
            "\n",
            "Summary statistics for BusinessAssetsValue (Progoti = 1):\n",
            "count    4.456000e+03\n",
            "mean     2.414987e+04\n",
            "std      6.443022e+04\n",
            "min      0.000000e+00\n",
            "25%      2.208407e+03\n",
            "50%      9.559292e+03\n",
            "75%      2.409911e+04\n",
            "max      1.672500e+06\n",
            "Name: BusinessAssetsValue, dtype: float64\n",
            "\n",
            "Summary statistics for BusinessAssetsValue (Census = 1):\n",
            "count    0.0\n",
            "mean     NaN\n",
            "std      NaN\n",
            "min      NaN\n",
            "25%      NaN\n",
            "50%      NaN\n",
            "75%      NaN\n",
            "max      NaN\n",
            "Name: BusinessAssetsValue, dtype: float64\n",
            "\n",
            "Summary statistics for BusinessAssetsValue (Dabi = 1 & followup = 0):\n",
            "count    0.0\n",
            "mean     NaN\n",
            "std      NaN\n",
            "min      NaN\n",
            "25%      NaN\n",
            "50%      NaN\n",
            "75%      NaN\n",
            "max      NaN\n",
            "Name: BusinessAssetsValue, dtype: float64\n",
            "\n",
            "Summary statistics for BusinessAssetsValue (Dabi = 1 & followup = 1):\n",
            "count    0.0\n",
            "mean     NaN\n",
            "std      NaN\n",
            "min      NaN\n",
            "25%      NaN\n",
            "50%      NaN\n",
            "75%      NaN\n",
            "max      NaN\n",
            "Name: BusinessAssetsValue, dtype: float64\n",
            "\n",
            "Summary statistics for BusinessAssetsValue (Dabi = 1 & followup = 2):\n",
            "count    0.0\n",
            "mean     NaN\n",
            "std      NaN\n",
            "min      NaN\n",
            "25%      NaN\n",
            "50%      NaN\n",
            "75%      NaN\n",
            "max      NaN\n",
            "Name: BusinessAssetsValue, dtype: float64\n",
            "Business assets value calculated and saved as 'panel_temp_with_BusinessAssetsValue.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_BusinessAssetsValue.csv')\n",
        "\n",
        "# Step 1: Calculate BusinessHours\n",
        "data['BusinessHours'] = data['tot_no_hours'] * data['tot_no_days'] * data['tot_no_months']\n",
        "\n",
        "# Label the variable (in comments, since pandas does not support variable labels)\n",
        "# BusinessHours = \"total # hours per year enterprise operates\"\n",
        "\n",
        "# Step 2: Calculate Business Costs\n",
        "data['Costs'] = data[['cost_wages', 'cost_repairs', 'cost_rent_sites', 'cost_rent_mach',\n",
        "                      'cost_trans', 'cost_electricity', 'cost_fuel', 'cost_stock']].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label the variable (in comments, since pandas does not support variable labels)\n",
        "# Costs = \"Total business costs over the last 12 months\"\n",
        "\n",
        "# Step 3: Handle missing values for Costs\n",
        "data.loc[data['HasBusiness'].isna(), 'Costs'] = np.nan\n",
        "\n",
        "# Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_BusinessHours_and_Costs.csv', index=False)\n",
        "\n",
        "print(\"Business hours and costs calculated and saved as 'panel_temp_with_BusinessHours_and_Costs.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oauZT8zh9Wb",
        "outputId": "cd6c5232-50b5-46eb-b662-a3f8c505c0ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Business hours and costs calculated and saved as 'panel_temp_with_BusinessHours_and_Costs.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_BusinessHours_and_Costs.csv')\n",
        "\n",
        "# Step 1: Calculate Esale2yr\n",
        "data['Esale2yr'] = ((40 * data['prob2_inc20']) + (10 * data['prob2_inc10']) +\n",
        "                    (0 * data['prob2_nochange']) + (-10 * data['prob2_dec10']) +\n",
        "                    (-40 * data['prob2_dec20'])) / 100\n",
        "\n",
        "# Step 2: Calculate SDsale2yr\n",
        "data['SDsale2yr'] = np.sqrt(\n",
        "    ((1600 * data['prob2_inc20']) + (100 * data['prob2_inc10']) +\n",
        "     (0 * data['prob2_nochange']) + (100 * data['prob2_dec10']) +\n",
        "     (1600 * data['prob2_dec20'])) / 100 - (data['Esale2yr'] ** 2)\n",
        ")\n",
        "\n",
        "# Step 3: Calculate CVsale2yr\n",
        "data['CVsale2yr'] = data['SDsale2yr'] / data['Esale2yr'].abs()\n",
        "\n",
        "# Label for CVsale2yr (in comments, as pandas does not support variable labels)\n",
        "# CVsale2yr = \"Coefficient of variation of expected sales growth in 2 years\"\n",
        "\n",
        "# Step 4: Tagging and calculating mean CVsale2yr by branch and followup for census data\n",
        "data['tag'] = data.groupby(['identifier_branch', 'followup'])['identifier_branch'].transform('size') == 1\n",
        "\n",
        "data['MCVsale2yr'] = data.groupby(['identifier_branch', 'followup'])['CVsale2yr'].transform('mean').where(data['census'] == 1)\n",
        "\n",
        "# Step 5: Calculate xCVsale2yr for baseline (followup == 0)\n",
        "data['xCVsale2yr'] = data['MCVsale2yr'].where(data['followup'] == 0)\n",
        "\n",
        "# Step 6: Calculate the maximum BMCVsale2yr by branch\n",
        "data['BMCVsale2yr'] = data.groupby('identifier_branch')['xCVsale2yr'].transform('max')\n",
        "\n",
        "# Step 7: Calculate the median of BMCVsale2yr for tagging HBMCVsale2yr\n",
        "BMCVsale2yr_median = data.loc[(data['tag'] == 1) & (data['followup'] == 0), 'BMCVsale2yr'].median()\n",
        "\n",
        "data['HBMCVsale2yr'] = data['BMCVsale2yr'] >= BMCVsale2yr_median\n",
        "\n",
        "# Label for HBMCVsale2yr (in comments, as pandas does not support variable labels)\n",
        "# HBMCVsale2yr = \"High expected demand uncertainty\"\n",
        "\n",
        "# Step 8: Drop intermediate variables\n",
        "data.drop(columns=['tag', 'Esale2yr', 'SDsale2yr', 'CVsale2yr', 'BMCVsale2yr', 'MCVsale2yr', 'xCVsale2yr'], inplace=True)\n",
        "\n",
        "# Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_HBMCVsale2yr.csv', index=False)\n",
        "\n",
        "print(\"Expected sales growth uncertainty calculated and saved as 'panel_temp_with_HBMCVsale2yr.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3TnWcYykI6K",
        "outputId": "be065d9f-5d6c-4d60-e569-042233691e17"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected sales growth uncertainty calculated and saved as 'panel_temp_with_HBMCVsale2yr.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_HBMCVsale2yr.csv')\n",
        "\n",
        "# Step 1: Calculate total household income\n",
        "data['HHIncome'] = data[['wage_inc_hh', 'wage_inc_resp', 'profit_oth_bus', 'Profits']].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label for HHIncome (in comments, as pandas does not support variable labels)\n",
        "# HHIncome = \"Household income\"\n",
        "\n",
        "# Step 2: Calculate owner's hours worked in the main household business\n",
        "data['HoursWorkedBuss'] = data['owner_months'] * data['owner_days_pm'] * data['owner_hrs_pd']\n",
        "data['HoursWorkedBuss'] = data['HoursWorkedBuss'].where(data['HasBusiness'] != 0, 0)\n",
        "\n",
        "# Label for HoursWorkedBuss (in comments)\n",
        "# HoursWorkedBuss = \"Owner's hours worked\"\n",
        "\n",
        "# Step 3: Calculate the value of Non-BRAC borrowing\n",
        "# NbracloanV? matches columns like 'NbracloanV1', 'NbracloanV2', etc.\n",
        "# Use regex to select columns with 'NbracloanV' prefix\n",
        "nbracloanv_columns = data.filter(regex=r'^NbracloanV\\d+').columns\n",
        "data['NonBRACLoanValue'] = data[nbracloanv_columns].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label for NonBRACLoanValue (in comments)\n",
        "# NonBRACLoanValue = \"Non-BRAC loan value\"\n",
        "\n",
        "# Step 4: Determine if any Non-BRAC loan exists\n",
        "data['NonBRACLoanD'] = data['NonBRACLoanValue'] > 0\n",
        "data['NonBRACLoanD'] = data['NonBRACLoanD'].where(data['NonBRACLoanValue'].notna(), np.nan)\n",
        "\n",
        "# Label for NonBRACLoanD (in comments)\n",
        "# NonBRACLoanD = \"Any Non-BRAC loan\"\n",
        "\n",
        "# Step 5: Calculate the total value of transfers or loans given\n",
        "# loan_given? matches columns like 'loan_given1', 'loan_given2', etc.\n",
        "# trans_given? matches columns like 'trans_given1', 'trans_given2', etc.\n",
        "trlendv_columns = data.filter(regex=r'^(loan_given|trans_given)\\d*').columns\n",
        "data['TrlendV'] = data[trlendv_columns].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label for TrlendV (in comments)\n",
        "# TrlendV = \"Transfers or loans given\"\n",
        "\n",
        "# Step 6: Calculate the total value of transfers received\n",
        "# trans_received? matches columns like 'trans_received1', 'trans_received2', etc.\n",
        "trans_received_columns = data.filter(regex=r'^trans_received\\d*').columns\n",
        "data['TransfersReceived'] = data[trans_received_columns].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label for TransfersReceived (in comments)\n",
        "# TransfersReceived = \"Transfers received\"\n",
        "\n",
        "# Step 7: Calculate net borrowing and transfers\n",
        "data['LoanTrNet'] = (data['BRACLoanValue'] + data['NonBRACLoanValue'] +\n",
        "                     data['TransfersReceived'] - data['TrlendV'])\n",
        "\n",
        "# Label for LoanTrNet (in comments)\n",
        "# LoanTrNet = \"Net borrowing and transfers\"\n",
        "\n",
        "# Step 8: Calculate total land size\n",
        "data['LandSize'] = data[['landS_cult', 'landS_mortgage', 'landS_other', 'landS_rentout', 'landS_sharecrop']].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label for LandSize (in comments)\n",
        "# LandSize = \"Size of land owned (decimals)\"\n",
        "\n",
        "# Step 9: Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_final.csv', index=False)\n",
        "\n",
        "print(\"All variables calculated and saved as 'panel_temp_final.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiQN-MpYkOOd",
        "outputId": "de0ecf23-49ec-4a6e-e564-e049b24c4cb9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All variables calculated and saved as 'panel_temp_final.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_final.csv')\n",
        "\n",
        "# Step 1: Calculate the total value of non-business household assets\n",
        "data['NonBusinessAssets'] = data[['hhasset_radio', 'hhasset_tv', 'hhasset_fan', 'hhasset_fridge',\n",
        "                                  'hhasset_cell', 'hhasset_bike', 'hhasset_motor', 'hhasset_jewel',\n",
        "                                  'hhasset_car', 'hhasset_oth1', 'hhasset_oth2', 'hhasset_oth3']].sum(axis=1, skipna=False)\n",
        "\n",
        "# Label for NonBusinessAssets (in comments, as pandas does not support variable labels)\n",
        "# NonBusinessAssets = \"Value of non-business household assets\"\n",
        "\n",
        "# Step 2: Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_NonBusinessAssets.csv', index=False)\n",
        "\n",
        "print(\"Non-business household assets calculated and saved as 'panel_temp_with_NonBusinessAssets.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRiY5aJ6ks8P",
        "outputId": "7d5f9eac-5c99-451c-d968-f586d7c90242"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-business household assets calculated and saved as 'panel_temp_with_NonBusinessAssets.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_NonBusinessAssets.csv')\n",
        "\n",
        "# Step 1: Replace negative food expenditure values with NaN\n",
        "data['exp_food'] = data['exp_food'].where(data['exp_food'] >= 0, other=np.nan)\n",
        "\n",
        "# Step 2: Calculate yearly food expenditure adjusted for Ramadan\n",
        "data['exp_food_year'] = data['exp_food'] * 48  # Adjusting for Ramadan\n",
        "\n",
        "# Step 3: Calculate temporary expenditure (monthly expenses multiplied by 12)\n",
        "data['tempexp'] = (data['exp_fuel'] * 12 + data['exp_electricity'] * 12 + data['exp_trans'] * 12 +\n",
        "                   data['exp_cosmetic'] * 12 + data['exp_maid'] * 12 + data['exp_entertain'] * 12)\n",
        "\n",
        "# Step 4: Calculate total expenditure\n",
        "# 'exp_other?' matches columns like 'exp_other1', 'exp_other2', etc.\n",
        "exp_other_columns = data.filter(regex=r'^exp_other\\d*').columns\n",
        "data['total_expenditure'] = data[['exp_food_year', 'tempexp', 'exp_clothes', 'exp_utensils',\n",
        "                                  'exp_furniture', 'exp_ceremony', 'exp_dowry', 'exp_educ'] + list(exp_other_columns)].sum(axis=1, skipna=False)\n",
        "\n",
        "# Handle cases where total_expenditure is zero\n",
        "data['total_expenditure'] = data['total_expenditure'].where(data['total_expenditure'] != 0, other=np.nan)\n",
        "\n",
        "# Label for total_expenditure (in comments, as pandas does not support variable labels)\n",
        "# total_expenditure = \"total expenditure last year\"\n",
        "\n",
        "# Step 5: Calculate per-capita expenditure (PCE)\n",
        "data['PCE'] = data['total_expenditure'] / data['HHsize']\n",
        "\n",
        "# Label for PCE (in comments)\n",
        "# PCE = \"Consumption per-capita\"\n",
        "\n",
        "# Step 6: Drop intermediate columns\n",
        "data.drop(columns=['exp_food_year', 'tempexp'], inplace=True)\n",
        "\n",
        "# Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_PCE.csv', index=False)\n",
        "\n",
        "print(\"Per-capita expenditure calculated and saved as 'panel_temp_with_PCE.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9jvBxY6k9Xt",
        "outputId": "672222ae-bc31-4796-d28b-ad33e2e23b90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-capita expenditure calculated and saved as 'panel_temp_with_PCE.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_PCE.csv')\n",
        "\n",
        "# Step 1: Conditionally replace `RevBestM` and `RevWorstM` based on `Rev_vary`\n",
        "data['RevBestM'] = data['RevBestM'].where(data['Rev_vary'] != 0, other=data['Revenues'] / 12)\n",
        "data['RevWorstM'] = data['RevWorstM'].where(data['Rev_vary'] != 0, other=data['Revenues'] / 12)\n",
        "\n",
        "# Step 2: Conditionally replace `RevBestM` and `RevWorstM` if `HasBusiness` is 0\n",
        "data['RevBestM'] = data['RevBestM'].where(data['HasBusiness'] != 0, other=0)\n",
        "data['RevWorstM'] = data['RevWorstM'].where(data['HasBusiness'] != 0, other=0)\n",
        "\n",
        "# Step 3: Calculate the range of monthly revenues\n",
        "data['RevRange'] = data['RevBestM'] - data['RevWorstM']\n",
        "\n",
        "# Label for RevRange (in comments, as pandas does not support variable labels)\n",
        "# RevRange = \"Range of monthly revenues\"\n",
        "\n",
        "# Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_RevRange.csv', index=False)\n",
        "\n",
        "print(\"Range of monthly revenues calculated and saved as 'panel_temp_with_RevRange.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZINokoG_lTaz",
        "outputId": "14b4a560-fdd3-4a1a-8777-1970ba2a0d69"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range of monthly revenues calculated and saved as 'panel_temp_with_RevRange.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_RevRange.csv')\n",
        "\n",
        "# Step 1: Initialize riskaversion to 0 where risk_choice1 == 2\n",
        "data['riskaversion'] = np.where(data['risk_choice1'] == 2, 0, np.nan)\n",
        "\n",
        "# Step 2: Replace riskaversion based on conditions\n",
        "data['riskaversion'] = np.where((data['riskaversion'].isna()) & (data['risk_choice2'] == 2), 1, data['riskaversion'])\n",
        "data['riskaversion'] = np.where((data['riskaversion'].isna()) & (data['risk_choice3'] == 2), 2, data['riskaversion'])\n",
        "data['riskaversion'] = np.where((data['riskaversion'].isna()) & (data['risk_choice4'] == 2), 3, data['riskaversion'])\n",
        "data['riskaversion'] = np.where((data['riskaversion'].isna()) & (data['risk_choice5'] == 2), 4, data['riskaversion'])\n",
        "data['riskaversion'] = np.where((data['riskaversion'].isna()) & (data['risk_choice5'] == 1), 5, data['riskaversion'])\n",
        "\n",
        "# Label for riskaversion (in comments, as pandas does not support variable labels)\n",
        "# riskaversion = \"risk aversion (0=low, 5=high)\"\n",
        "\n",
        "# Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_riskaversion.csv', index=False)\n",
        "\n",
        "print(\"Risk aversion calculated and saved as 'panel_temp_with_riskaversion.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGs9A66UlpwU",
        "outputId": "10270a4c-c297-4460-b98e-2811e9875eaf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk aversion calculated and saved as 'panel_temp_with_riskaversion.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_riskaversion.csv')\n",
        "\n",
        "# Step 1: Initialize patience to NaN (equivalent to Stata's missing value '.')\n",
        "data['patience'] = np.nan\n",
        "\n",
        "# Step 2: Replace patience based on the conditions provided\n",
        "data['patience'] = np.where(data['time_pref6'] == 1, 1, data['patience'])\n",
        "data['patience'] = np.where((data['patience'].isna()) & (data['time_pref6'] == 2) & (data['time_pref5'] == 1), 2, data['patience'])\n",
        "data['patience'] = np.where((data['patience'].isna()) & (data['time_pref6'] == 2) & (data['time_pref5'] == 2) & (data['time_pref4'] == 1), 3, data['patience'])\n",
        "data['patience'] = np.where((data['patience'].isna()) & (data['time_pref6'] == 2) & (data['time_pref5'] == 2) & (data['time_pref4'] == 2) & (data['time_pref3'] == 1), 4, data['patience'])\n",
        "data['patience'] = np.where((data['patience'].isna()) & (data['time_pref6'] == 2) & (data['time_pref5'] == 2) & (data['time_pref4'] == 2) & (data['time_pref3'] == 2) & (data['time_pref2'] == 1), 5, data['patience'])\n",
        "data['patience'] = np.where((data['patience'].isna()) & (data['time_pref6'] == 2) & (data['time_pref5'] == 2) & (data['time_pref4'] == 2) & (data['time_pref3'] == 2) & (data['time_pref2'] == 2) & (data['time_pref1'] == 1), 6, data['patience'])\n",
        "data['patience'] = np.where((data['patience'].isna()) & (data['time_pref6'] == 2) & (data['time_pref5'] == 2) & (data['time_pref4'] == 2) & (data['time_pref3'] == 2) & (data['time_pref2'] == 2) & (data['time_pref1'] == 2), 7, data['patience'])\n",
        "\n",
        "# Label for patience (in comments, as pandas does not support variable labels)\n",
        "# patience = \"patience (1=low, 7=high)\"\n",
        "\n",
        "# Save the updated dataset\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_patience.csv', index=False)\n",
        "\n",
        "print(\"Patience variable calculated and saved as 'panel_temp_with_patience.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhmJNWQhmE9X",
        "outputId": "6d2a1828-8c7d-45d8-e09f-460381e328a5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patience variable calculated and saved as 'panel_temp_with_patience.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset if it's not already loaded\n",
        "data = pd.read_csv('/content/drive/My Drive/Rep/Test/panel_temp_with_patience.csv')\n",
        "\n",
        "# Step 1: Impute baseline values for the specified variables\n",
        "variables_to_impute = ['agrisec', 'hireLanyE', 'loans_taken', 'retail', 'startbus']\n",
        "\n",
        "for var in variables_to_impute:\n",
        "    # Create the imputed baseline variable (b_var)\n",
        "    data[f'b_{var}'] = data.groupby('id')[var].transform('max')\n",
        "\n",
        "# Step 2: Label the variables (in comments, since pandas does not support labels)\n",
        "# b_agrisec = \"Sector: Agriculture-related\"\n",
        "# b_hireLanyE = \"Wants to hire new worker\"\n",
        "# b_loans_taken = \"=1 if at baseline anyone in hh had ever taken out any loan\"\n",
        "# b_retail = \"Sector: Retail\"\n",
        "# b_startbus = \"Wants to start business\"\n",
        "\n",
        "# Step 3: Sort the DataFrame by 'id' and 'followup'\n",
        "data = data.sort_values(by=['id', 'followup'])\n",
        "\n",
        "# Step 4: Reorder columns alphabetically, except for some key variables\n",
        "# Get the list of columns in alphabetical order\n",
        "all_columns_sorted = sorted(data.columns)\n",
        "\n",
        "# Define the key variables that should appear at the top\n",
        "key_variables = ['id', 'region', 'identifier_branch', 'brac_loanee_vo_no', 'followup', 'endline', 'treatment', 'dabi', 'progoti', 'census']\n",
        "\n",
        "# Create the final column order\n",
        "final_column_order = key_variables + [col for col in all_columns_sorted if col not in key_variables]\n",
        "\n",
        "# Reorder the DataFrame columns\n",
        "data = data[final_column_order]\n",
        "\n",
        "# Step 5: Save the dataset to a new file\n",
        "data.to_csv('/content/drive/My Drive/Rep/Test/Data_for_analysis.csv', index=False)\n",
        "\n",
        "print(\"Imputed baseline values, sorted and reordered columns, and saved the dataset as 'Data_for_analysis.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUEJrKGgnis6",
        "outputId": "0a1158a1-7fec-4864-bb9f-64f2d8d5223e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputed baseline values, sorted and reordered columns, and saved the dataset as 'Data_for_analysis.csv'.\n"
          ]
        }
      ]
    }
  ]
}